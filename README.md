# ExpertTransformer
Expert Transformer (ET) is a fine-grained Mixture-of-Experts (MoE) building block for Transformer architectures that enables specialization at both the sequence and token levels. The module replaces standard self-attention and feed-forward sublayers with routed attention experts and routed FFN experts
